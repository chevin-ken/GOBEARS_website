<!DOCTYPE html>
<html lang="en">
  <head>
    <title>GOBEARS EE106A Final Project</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    <link href="https://fonts.googleapis.com/css?family=Work+Sans:100,200,300,400,700,800" rel="stylesheet">

    <link rel="stylesheet" href="css/open-iconic-bootstrap.min.css">
    <link rel="stylesheet" href="css/animate.css">
    
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">
    <link rel="stylesheet" href="css/magnific-popup.css">

    <link rel="stylesheet" href="css/aos.css">

    <link rel="stylesheet" href="css/ionicons.min.css">

    <link rel="stylesheet" href="css/bootstrap-datepicker.css">
    <link rel="stylesheet" href="css/jquery.timepicker.css">

    
    <link rel="stylesheet" href="css/flaticon.css">
    <link rel="stylesheet" href="css/icomoon.css">
    <link rel="stylesheet" href="css/style.css">
    <script
      src="https://code.jquery.com/jquery-3.3.1.js"
      integrity="sha256-2Kok7MbOyxpgUVvAk/HJ2jigOSYS2auK4Pfzbm7uH60="
      crossorigin="anonymous">
    </script>
    <script> 
      $(function(){
        $("#navbar").load("navbar.html"); 
        $("#footer").load("footer.html"); 
      });
    </script> 
  </head>
  <body>
    
    <div id="navbar"></div>
    <!-- END nav -->
    
    <section class="home-slider ftco-degree-bg">
      <div class="slider-item">
        <div class="overlay"></div>
        <div class="container">
          <div class="row slider-text align-items-center justify-content-center">
            <div class="col-md-12 ftco-animate text-center">
              <h1 class="mb-4">GOBEARS:
                <strong class="typewrite" data-period="500" data-type='["Garden Optimizing Botanical Earth Actuating Robotics System"]'>
                  <span class="wrap"></span>
                </strong>
              </h1>
              <p>Group 41: Kevin Chen, Anthony Ding, Victor Ho, Jasmine Wang</p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- END slider -->

    <section class="ftco-section" id="part1">
      <div class="container">
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h2>Introduction </h2>
            <p> GOBEARS (Garden Actuating Botanical Earth Actuating Robotics System) is a robot designed to perform common gardening tasks autonomously. GOBEARS can use a trowel to dig a hole, it can pick up a plant and plant it into a hole, and it can pick up a watering can and 
              use it to water the plant. 
            </p>
            <p>This is an interesting project because gardening is not an area that comes to mind when you think of robotics, compared to the more mainstream fields like self driving, drones, or human robotic interaction. However, we feel that robotics has a great application in 
              gardening, and the sensing, planning, and actuation concepts we learned in class are very applicable. For example, we need to have sensing in order to detect the poses of our gardening tools and objects. We also need to calculate optimal poses that allow 
              for the ability to pick up gardening objects and perform movements while holding these objects, which involves using inverse kinematics to plan trajectories. Finally, we need to actuate these plans in order to achieve the motions necessary to perform 
              the gardening tasks. Also, there are some interesting motions such as digging and watering that require creativity in designing trajectories, which makes the gardening problem more interesting to solve.
            </p>
            <p>GOBEARS has clear application in the gardening world, as gardening is quite an arduous task sometimes, so having an autonomous robot do it for you is quite appealing. An intelligent, durable, and robust gardening bot would be able to revolutionize the gardening 
              industry. With more research done on the intersection of robotics and gardening, gardening robots could also perform a much wider variety of tasks to a higher quality. At the same time though, it is important to consider that autonomous robots do pose a threat to 
              the jobs of gardening industry workers, which echoes the sentiment of the debate about the impact of autonomous robots on jobs that has become much more prevalent over the past couple of decades.
            </p>
          </div>
        </div>
    </section>

    <section class="ftco-section" id="part2">
      <div class="container">
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h2>Design</h2>
            <p>What design criteria must your project meet? What is the desired functionality?</p>
            <p>Describe the design you chose</p>
            <p>What design choices did you make when you formulated your design? What trade-offs did you
              have to make?</p>
            <p>How do these design choices impact how well the project meets design criteria that would be
              encountered in a real engineering application, such as robustness, durability, and efficiency?</p>
          </div>
        </div>
      </div>
    </section>

    <section class="ftco-section" id="part3">
      <div class="container">
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h2>Implementation</h2>
          </div>
          <br>
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h5>Hardware</h5>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>The hardware setup for GOBEARS was fairly straightforward. We used Baxter as the robot, Baxter right hand for the camera, and Baxter left arm with the gripper attachment for movement. Here is what our workplace setup looks like.</p>
          </div>
        </div>
        <!-- Workspace setup diagram  -->
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
            <div class="media block-10 d-block text-center">
              <div class="member-card d-flex justify-content-center">
                <img src="images/digworkspace.png" width=300px height=auto>
              </div>
              <p>Dig Workspace</p>
            </div>
          </div>
        </div>
        <br>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
            <div class="media block-10 d-block text-center">
              <div class="member-card d-flex justify-content-center">
                <img src="images/digworkspace2.jpg" width=300px height=auto>
              </div>
              <p>Gripping the Trowel before Digging</p>
            </div>
          </div>
        </div>
        <br>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
            <div class="media block-10 d-block text-center">
              <div class="member-card d-flex justify-content-center">
                <img src="images/plantworkspace.png" width=300px height=auto>
              </div>
              <p>Plant Workspace</p>
            </div>
          </div>
        </div>
        <br>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
            <div class="media block-10 d-block text-center">
              <div class="member-card d-flex justify-content-center">
                <img src="images/plantworkspace2.jpg" width=300px height=auto>
              </div>
              <p>Picking up the Plant</p>
            </div>
          </div>
        </div>
        <br>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
            <div class="media block-10 d-block text-center">
              <div class="member-card d-flex justify-content-center">
                <img src="images/waterworkspace.png" width=300px height=auto>
              </div>
              <p>Water Workspace</p>
            </div>
          </div>
        </div>
        <br>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
            <div class="media block-10 d-block text-center">
              <div class="member-card d-flex justify-content-center">
                <img src="images/waterworkspace2.jpg" width=300px height=auto>
              </div>
              <p>Picking up the Watering Can</p>
            </div>
          </div>
        </div>
        <br>
        <!-- Talk about trowel modification, and other substituted parts (like plant, water, dirt substitutions) -->
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h5>Parts</h5>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Our solution consisted of several items that are used in each gardening action. Each one of these items has an AR tag attached to it in order for Baxter to be able to detect the relative position of these objects using AR tracking. </p>
            <p>In the first action, digging, we needed a trowel and a dirt bin to dig out of. 
              For the trowel, we built a gripper add-on using cardboard that would help Baxter be able to grip the trowel firmly, and prevent tilting of the trowel along its major axis. 
              The attachment consists of two large carboard square pieces with two smaller cardboard pieces that are attached perpendicular to the square pieces. The attachment is then fortified with tape. 
              These smaller square pieces serve to resist torque from gravity while the trowel is being held by Baxter. For the dirt bin, we used a plastic box that was filled with dried beans. We chose to use
              dried beans instead of actual dirt simply for lab cleanliness reasons, as it is easier to pick up beans from the ground compared to dirt. This was definitely a good choice, as we had to pick up
              beans from the ground multiple times in the lab while fine-tuning our digging motion. </p>
           </div>
        </div>
        <!-- Trowel diagram with modified gripper -->
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
            <div class="media block-10 d-block text-center">
              <div class="member-card d-flex justify-content-center">
                <img src="images/trowel.png" width=200px height=auto>
              </div>
              <p>Trowel with gripper attachment</p>
            </div>
          </div>
        </div>
        <br>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
            <div class="media block-10 d-block text-center">
              <div class="member-card d-flex justify-content-center">
                <img src="images/bin.png" width=300px height=auto>
              </div>
              <p>Dirt Bin</p>
            </div>
          </div>
        </div>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>In the next action, planting, we used a green squishy toy to emulate a plant. We chose this object because it was relatively easy to grip, and because it was green. </p>
           </div>
        </div>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
            <div class="media block-10 d-block text-center">
              <div class="member-card d-flex justify-content-center">
                <img src="images/plant.png" width=200px height=auto>
              </div>
              <p>Plant</p>
            </div>
          </div>
        </div>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>In the last action, watering, we used a plastic water bottle to emulate a watering can. We used a plastic water bottle because it can also be gripped by Baxter, and it is light enough when it is about a third of the way full for Baxter to tilt it downwards when watering the plant. 
              We also did not want to pour actual water in a lab due to lab safety reasons, so we kept the water bottle closed throughout.
            </p>
           </div>
        </div>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
            <div class="media block-10 d-block text-center">
              <div class="member-card d-flex justify-content-center">
                <img src="images/water.png" width=200px height=auto>
              </div>
              <p>Watering Can</p>
            </div>
          </div>
        </div>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h5> Software </h5>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p> The software we used for the project is built using ROS and python. 
              The overall structure involves running many lower level nodes that perform basic functionality such as vision and control, and then writing
              our own logic wrapper over these nodes to actually perform our gardening actions.
            </p> 
            <p> For lower level nodes, we used some files from Baxter packages. These include baxter_grippers.launch from baxter_move_it_config to handle gripper control,
              joint_trajectory_action_server.py from baxter_interface to handle actuation, and camera_control.py from baxter_tools to enable the right hand camera.
              We also used the ar-track_alvar.launch launchfile from the ar_track_alvar package in order to enable AR tag tracking. We modified the launch file 
              slightly such that it subscribes to the right hand camera topics for image and camera intrinsics information, and also publishes the poses of the AR tags
              with respect to the base frame by subscribing to a tf static transform publisher.
            </p>
            <p> For our higher level wrapper code, we also subdivided that into several levels of modularity. We first copied over the motion planner that we used in 
              lab 7 to handle our inverse kinematics path planning, as well as plan execution. Then, we defined a Baxter class that directly interfaces with the 
              path planner. Baxter includes a move to pose method that takes in a pose and runs the path planner to plan and execute a trajectory to this pose. 
              Baxter also handles gripper control, which includes commands such as open, close, and calibrate gripper. Baxter provides vision functions as well, including 
              scan and get AR tag pose. Baxter has a node that subscribes /ar_pose_marker, which is the topic that ar_track_alvar publishes AR tag pose information to.
              During scan, Baxter will retrieve all AR tag information necessary in order to complete its current task and store the poses. Then, when the poses are needed,
              Baxter provides the get AR tag pose function which returns the pose of the requested AR tag.
            </p>
            <p> Our highest level of abstraction is the GOBEARS class, which interfaces with the Baxter class in order to perform the gardening actions of 
              digging, planting, and watering. We defined relative poses to each AR tag that Baxter needs to reach in order to accomplish each step. GOBEARS will 
              execute trajectories by retrieving AR tag poses from Baxter, transform the relative poses from the AR tag frames to the base frame using homogenous transforms,
              and then plan and execute trajectories to these poses by using Baxter's move to pose method. It will also call Baxter's close and open gripper methods whenever necessary, and will call Baxter's scan method between each task in order to get the AR tag poses needed for each task.
              In order to calculate the 4x4 homogenous transform matrices, we create a 
              geometry_msgs.Transform object using the orientation and position of the AR pose, call scipy's from_quat method to turn the orientation quaternion into a 3x3 rotation matrix, and then
              compose the 4x4 matrix using the 3x3 rotation matrix and the translation vector.
              
            </p>
           </div>
        </div>
        <!-- Code structure diagram -->
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
            <div class="media block-10 d-block text-center">
              <div class="member-card d-flex justify-content-center">
                <img src="images/code_diagram.png" width=500px height=auto>
              </div>
              <p>Code structure diagram</p>
            </div>
          </div>
        </div>
        <br>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h5> Runtime Steps </h5>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate"></div>
            <h6> Step 1: Setup </h6>
            <p> Run the low level nodes, which are the joint_trajectory_action_server, camera_control for the right hand camera, baxter_grippers launchfile, and the ar_track_alvar launchfile. 
              Run RVIZ to visualize the AR tags and planned trajectory animations.</p> 
            <h6> Step 2: Baxter Initialize </h6> 
            <p> Initializing Baxter involves several steps. First, it will initialize a path planner object for its left arm. It will also create a box obstacle for the table with the planner to ensure that
              the trajectories computed by the path planner will not cause collisions with the table. Then, it will initialize a subscriber that subscribes to 
              the /ar_pose_marker topic in order to receive AR pose information. It will wait for the callback to finish updating its stored list of AR tag positions, and then will complete initialization by calibrating the left gripper.
            </p>
            <h6> Step 3: Dig</h6> 
            <p> First, we will make sure to place the trowel and the dirt bin in locations on the table that can be reached within Baxter's workspace. We will move Baxter left arm to a predefined reset pose. We run scan again to confirm that we have the positions of the trowel and 
              dirt bin. Then, we move to a hover pose above the trowel, and then move to a pickup pose where the gripper is in position to pick up the trowel. Note that all predefined poses that we mention are poses relative to the AR tag 
              that we retreived using tf echo. In order to actually move Baxter's left arms to the desired positions relative to the AR tag, we need to compute a homogenous transform
              to transform the relative poses from the AR tag frame to the base frame. Also, the purpose of the hover poses is to ensure that we don't knock over the objects we are trying to pick up when 
              moving to the next pickup pose. We close the gripper and then move to a hover position above the dirt bin such that Baxter
              is now holding the trowel right over the dirt bin, angled downwards. We can then lower the trowel such that it is digging into the beans. We then execute a shoveling motion by performing a trajectory where we increase the pitch 
              of the gripper to tilt it, and then move the shovel forward in the bin such that it pushes the beans out from the center, effectively creating a hole. We enforce orientation constraints throughout this 
              motion so that the trowel continues to be angled in a way that it can displace the beans. Last, we move the trowel back to its original starting position, and release the gripper.
            </p>
            <h6> Step 4: Plant </h6>
            <p> We clear the trowel from the workspace and put the plant on the table. We will start by moving to the same reset pose from the previous step and run scan to confirm that we have the positions of the plant and the dirt bin. Then, we
              move to a hover pose about the plant, and move downwards into a pickup pose where we close the gripper and pick up the plant. Next, we move to a hover pose above the dirt bin and then move
              straight down so that the plant is above the hole we created in the previous step. We then release the gripper so the plant is placed in the hole and move back upwards to the hover pose above the 
              dirt bin. Throughout these motions, we maintain an orientation where the gripper is pointed straight down.
            </p>
            <h6> Step 5: Water </h6>
            <p> We clear the plant from the workspace and put the water bottle on the table. Once again, we move to the reset position and run scan to confirm that we have the positions of the water bottle and the dirt bin. 
              Then, we move to a water pickup hover pose, which is actually different from the hover poses in the previous steps. Whereas before the hover poses were above the object, in the case of the water bottle, we want to
              pick up the water bottle from the side, so we need to move to a pose where the gripper is perpendicular to the major axis of the water bottle. Then, we can move the gripper forward to the water pickup pose and close the gripper to grip the 
              water bottle. Then, we move the water bottle to a hover pose above the dirt bin. Here, we enforce orientation constraints to prevent the water bottle from tilting over,
              as in a real life gardening situation, we don't want our watering can to spill out its water before reaching its desired pose. Then, we move to our next watering pose, 
              in which Baxter will tilt the water bottle forward to simulate watering the plant. Finally, we will move back to the water pickup hover pose, followed by the water pickup pose and then open the gripper to return
              the water bottle to its original position.
            </p>
          </div>
        </div>
      </div>
    </section>

    <section class="ftco-section" id="part4">
      <div class="container">
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h2>Results</h2>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Our project worked pretty well for the most part, as we were able to perform the three gardening tasks of digging, planting, and watering. 
            </p>
            <p>For digging, we were able to precisely position the gripper to
              pick up the trowel and to manuever the trowel into a pose where it could scoop the beans from the bin. The digging motion was not the most optimal, as we were unable to actually displace as many beans as we would
              have liked due to limitations of the length and depth of the bin, as well as the reachable workspace of Baxter. However, it still resembled the digging motion enough to be considered a success. We were
              also able to accurately return the trowel to its starting position.</p>
            <p>Our planting motion worked very well, as we were able to accurately detect the location of the plant, and move it precisely to the location of the hole in the bin.</p>
            <p>Watering the plant was also a relatively successful motion, as we were able to successfully grip the water bottle, move it over the bin while maintaining orientation constraints to ensure no spilling, and tilt the
              water bottle so that it simulates a pouring motion. 
            </p>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
            <div class="media block-10 d-block text-center">
              <div class="member-card d-flex justify-content-center">
                <iframe width="560" height="315" 
                src="https://www.youtube.com/embed/4vJ-u8Rg6ec" 
                title="YouTube video player" 
                frameborder="0" 
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
              </iframe>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="ftco-section" id="part5">
      <div class="container">
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h2>Conclusion</h2>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate"></div>
            <h6>Results Discussion</h6>
            <p>
            	Discuss your results. How well did your finished solution meet your design criteria?
            	At a fundamental level, our finished solution met the main design criteria we had. We were able to detect the variable positions of the different tools that the Baxter had to interact with, we could determine the pose necessary to then allow the Baxter to properly pick up or use/interact with those tools, and we were able to achieve the main motions of digging, planting and watering successfully. <br>
            	However, there were still multiple areas where we felt that the robustness of our solution was a little lackluster. For instance, the dependence on AR markers to determine the location and orientation of the various tools the Baxter interacts with is pretty restrictive, even though the tools don't need to be placed exactly on the AR tags. There are very few practical scenarios when such a convenient crutch is available. Even if an AR tag could be used in a pratical scenario, so many factors such as the exposure of the camera, whether enough lighting is available for the camera to see the AR tags, or if the tools are too far from the camera could all cause our solution to fail. In addition, the scalability of having to specifically AR tag and identify the appropriate homogeneous transform between the AR tag frame and the end-effector position to pick up each specific tool isn't great. Overall, while we did meet the the underlying design requirements, there are areas where we hope to improve the design and make it more robust and tolerable to different conditions and failures which will be discussed below.
            </p>
            <h6>Challenges and Difficulties</h6>
            <p>
            	Throughout the process of working on this project, we faced a lot of challenges and difficulties along the way just like when working on any other project. Ranging from the robot not being able to pick up the shovel properly to not being able to find the python documentation for some packages in ROS. However, there were three main challenges that were present throughout the entire duration of the project and even to the very end. <br>

            	The first issue was the workspace limitations and the size of the Baxter. Because of the length and thickness of Baxter's links, there are some configurations that Baxter can't enter. While this isn't normally much of an issue on its own, when the region we could place the tools we wanted to interact with was limited to a 1 ft wide table that was quite a lot shorter than the Baxter's first joint, the issues become more apparent. For some objects the overlap between the set of poses necessary to pick up an object sitting on this table and the poses that the object can reach is actually quite a limited space. The water bottle in particular was quite difficult for the Baxter to pick up becuase of how low the gripper had to be combined with the required horizontal orientation of the gripper. The size of Baxter's arms ultimately required the water bottle to be at the far tip of the table in order to have a configuration that can reach that orientation. Similarly, the trowel had to be relatively closer to Baxter for a successful pickup due to its slanted holder.<br>

            	Another major challenge was working with MoveIt. In general we found that the reliability of MoveIt was a little questionable. There were cases where even for a simple motion completely within the workspace, something that we had easily visualized using the zero-g mode of Baxter, the IK solver in MoveIt would return a super obscure path plan rotating some joints of the robot 360 degrees, moving the long way to some desired end configuration or sometimes even no plan at all. This caused us a lot of difficulty until the very end of the project, where the only solution we could really find to this was to specificy intermediate waypoints between the start and goal poses, apply orientation constraints where we could, and ultimately just hope that the Baxter would find a good path if any.<br>

            	Finally, an issue we faced while working on the project was in the scheduling of time to work on the machines. Within the last week and a half before the demo, when all the teams assigned to each machine wanted to maximize their worktime on these robots. Ultimately almost every hour of the day got booked, there were very limited time slots available to be booked causing us to have to go in to lab at rather unfortunate hours (on multiple days) to even get time to work and test on the Baxters themselves. There was also overhead each time we started and finished a session. We needed to retrace some previous steps to figure out where the group last left off, we had to spend time to undo a previous groups setup and then setup our gripper and workspace.
            </p>
        	<h6>Further Improvement and Future Possibilities</h6>
            <p>
            	Given the amount of time available, we are proud with the state of the solution, however, if given more time there are a couple flaws we would like to address, as well as future improvements that we would like to incorporate. 
            	One main flaw of our solution is that it was heavily reliant on the existing MoveIt package for its IK solver and path planner, which may or may not be able to find a solution to go from the current pose to the goal pose. The hacky way we sought to get around this issue was to continuously try to find the plan, and keep looping until it found one. While this helped prevent our code from erroring in most instances where the plan could be found in 1 or 2 attempts, it also introduced the vulnerability where in the worst case that the path planner keeps finding no possible plan. <br>
            	We had intended to fix this initially by establishing a number of attempts before the code would reset to a neutral position before trying again. However in doing so, this made it more common for the path planner to choose obscure paths while still not guaranteeing that a plan even exists since these neutral poses could be even less comvenient. We ran out of time before finding an appropriate solution, but a definite improvement is to hopefully eliminate the possibillity of not being able to find a path plan. Our plans to address this issue in the future would be try different reset positions such as maybe an intermediate position (average between start and goal?) rather than a purely neutral position as it maybe more easy to find a plan to the final pose. <br>
            	Another area in which we hope to improve our solution is to incorporate additional obstacles during the path planning stage of execution that represent the different tools in the workspace. In its current state, our solution is not spatially aware of the tools present on the table and is relying on the low profile of these tools and high goal poses to successfully avoid knocking over other tools. However, as the process scales up, tools become larger, more tools are introduced, we will need to plan around the physical presense of these tools and not knock them over. <br>
            	Finally, the biggest change we wanted to implement was the transition over from depending on AR tags for each tool to using AR tags to frame a workspace, where we then use CV to outline and identify different tools as well as the necessary orientations to interact with these tools. While we were successful in being able to give a rough bounding box for different tools based on CV already, we were unable to implement the basic orientation identification required, especially for a pose as specific as the +- 0.5 cm margin of error we have for the trowel's grip attachment. 
        	</p>
            <h6>Social Implications and Bigger Picture</h6>
            <p>
            	&emsp Looking at the GOBEARS system and how it fits in to the larger picture, it's pretty evident that in it's current state there is a low probability that it would cause any real consequential impacts from either social, political, or economic perspective. However, if the GOBEARS were to be truly refined to the state of commercial viability, then undoubtedly various concerns would arise. <br>
            	&emsp Seeing as how the GOBEARS system is currently intended for automating gardening, it's well within the realm of expectation that something like this could be repurposed for the automating larger and much more prevalent field of agriculture. Especially considering that large-scale agriculture needs to be much less precise than gardening and requires more labor and the nature of automating takss, there might even be a bigger market for the GOBEARS system in agriculture than gardening. Given this scenario, then the possible social, political, and economic implications of the GOBEARS system starts to line up with that of automation as a whole. <br>
            	&emsp The immediate primary consideration has to be the jobs of field workers which the GOBEARS system could automate. Especially for agriculture-dependent states in the US or even countries whose primary exports and GDP heavily depends on agriculture, GOBEARS could lead to the loss of work and work opportunity for many individuals, quickly devastating the economy in those areas. In order to asseass the true impacts of the loss of jobs that occurs with any automation, one also has to consider where the flow of labor would go, and what alternative income paths are available for those who are trained in the area that has been automated. The sudden transition of workers from agriculture to occupations that require less training could also drive down wages in those areas as well due to increased labor supply. This becomes a very real possibility as more specialized training and further education is expensive and a sudden drop or freeze in income is often undesirable or difficult to accept for those who were not prepared or have large families. Without intervention, this sudden shift in labor supply or unemployed individuals would lead to heavy dissatisfaction and could cause economic instability.<br>
            	&emsp While the economic instability from the automation of one field or another may be negligible on its own, but as engineering progresses and automation begins to penetrate more industries, many countries may need to implement additional policies to help lessen that impacts on the employees of those industries if not re-evaluate some long-standing fundamental economic policies.<br>
            	&emsp On the same note of automation and mass production, if the GOBEARS system ends up assuming a significant role in agriculture and is mass-produced on a large scale, one needs to consider the environmental impacts of such manufacutring and energy consumption as well. In such a case, ideally the GOBEARS system could look to transition its main energy source to renewable energies while being able to implement cheap recycle or repair programs to incentivize re-using parts rather than just producing more.
            </p>
          </div>
        </div>
      </div>
    </section>

    <section class="ftco-section" id="part6">
      <div class="container">
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h2>Team</h2>
          </div>
        </div>
        <br>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h4>Kevin Chen</h4>
            <br>
            <h6>Bio</h6>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Kevin is a 4th year undergraduate studying EECS at UC Berkeley. He is planning to graduate May 2022 and will be working full time as a software at Riot Games. 
              Kevin's main focuses in computer science include machine learning, backend software development, and web development. 
              He is a project leader and webmaster in Launchpad, a machine learning club at Berkeley. Outside of
              computer science, Kevin enjoys playing the saxophone in the UC Berkeley Wind Ensemble, playing Teamfight Tactics, and watching the NBA and NFL (big fan of Philadelphia Eagles and 76ers).
            </p>
          </div>
          <br>
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h6>Contributions</h6>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Kevin first came up with the idea of a gardening bot for the final project. He designed a lot of the software architecture for the GOBEARS source code and implemented much of the general logic
              for the main tasks. He also designed the final report website template and populated many sections in the website content.
            </p>
          </div>
          <!-- Profile Pic -->
          <div class="row justify-content-center mb-4 pb-4">
            <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
              <div class="media block-10 d-block text-center">
                <div class="member-card d-flex justify-content-center">
                  <img src="images/kevin_profile.jpg" class="img-part2-texture">
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h4>Anthony Ding</h4>
            <br>
            <h6>Bio</h6>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Insert Bio Here</p>
          </div>
          <br>
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h6>Contributions</h6>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Insert Contributions Here</p>
          </div>
          <!-- Profile Pic -->
          <div class="row justify-content-center mb-4 pb-4">
            <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
              <div class="media block-10 d-block text-center">
                <div class="member-card d-flex justify-content-center">
                  <img src="images/quilting/quilt_cut/template.png" class="img-part2-texture">
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h4>Victor Ho</h4>
            <br>
            <h6>Bio</h6>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Victor is a 4th year undergraduate studying EECS at UC Berkeley. He is planning to graduate May 2022  to work for a few years before looking to pursue a graduate degree. 
              Victor has experience in both electrical engineering, primarily in digital design, and computer science, focusing on backend software development.
              In his free time Victor likes playing videogames with friends, going hiking, and cross-country running; he has run more than 2,892 miles (longest linear distance across the contiguous US) since the start of college.
          	</p>
          </div>
          <br>
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h6>Contributions</h6>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Victor came up designs for how the Baxter grippers would interact with objects that are difficult to grip using just parallel finger grippers, eventually deciding on the grip for the trowel. He helped come up with the strategy using AR tags, hand cameras, and homogeneous transforms for Baxter to be able to position and orient its gripper correctly to interact with the various tools.</p>
          </div>
          <!-- Profile Pic -->
          <div class="row justify-content-center mb-4 pb-4">
            <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
              <div class="media block-10 d-block text-center">
                <div class="member-card d-flex justify-content-center">
                  <img src="images/victor_profile.jpg" class="img-part2-texture">
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h4>Jasmine Wang</h4>
            <br>
            <h6>Bio</h6>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Jasmine is a 4th year undergrad CS major at UC Berkeley. She is planning on graduating May 2022. Jasmine's interests in computer science are focused around machine learning, particularly 
              computer vision, as well as lower level programming and optimization. In her free time, Jasmine enjoys playing video games, travelling, and going on outings with friends. </p>
          </div>
          <br>
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h6>Contributions</h6>
          </div>
          <div class="col-md-12 text-left heading-section ftco-animate">
            <p>Jasmine helped design and code the AR tag to object transform abstractions, the path planning/constraints, etc. She (like everyone else in the group) also helped design/code many portions of the project, as for the most part everyone went into lab together and worked together.</p>
          </div>
          <!-- Profile Pic -->
          <div class="row justify-content-center mb-4 pb-4">
            <div class="col-md-12 d-flex align-self-stretch ftco-animate justify-content-center">
              <div class="media block-10 d-block text-center">
                <div class="member-card d-flex justify-content-center">
                  <img src="images/jasmine.jpg" class="img-part2-texture">
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="ftco-section" id="part7">
      <div class="container">
        <div class="row justify-content-center mb-4 pb-4">
          <div class="col-md-12 text-center heading-section ftco-animate">
            <h2>Additional Materials</h2>
            <p>Github: https://github.com/chevin-ken/GOBEARS</p>
            <p>Launch files: see ar_track_alvar.launch in the ar_track_alvar module on Github. </p>
          </div>
        </div>
      </div>
    </section>

    <div id="footer"></div>
  
  <!-- loader -->
  <div id="ftco-loader" class="show fullscreen"><svg class="circular" width="48px" height="48px"><circle class="path-bg" cx="24" cy="24" r="22" fill="none" stroke-width="4" stroke="#eeeeee"/><circle class="path" cx="24" cy="24" r="22" fill="none" stroke-width="4" stroke-miterlimit="10" stroke="#F96D00"/></svg></div>


  <script src="js/jquery.min.js"></script>
  <script src="js/jquery-migrate-3.0.1.min.js"></script>
  <script src="js/popper.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
  <script src="js/jquery.easing.1.3.js"></script>
  <script src="js/jquery.waypoints.min.js"></script>
  <script src="js/jquery.stellar.min.js"></script>
  <script src="js/owl.carousel.min.js"></script>
  <script src="js/jquery.magnific-popup.min.js"></script>
  <script src="js/aos.js"></script>
  <script src="js/jquery.animateNumber.min.js"></script>
  <script src="js/bootstrap-datepicker.js"></script>
  <script src="js/jquery.timepicker.min.js"></script>
  <script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyBVWaKrjvy3MaE7SQ74_uJiULgl1JY0H2s&sensor=false"></script>
  <script src="js/google-map.js"></script>
  <script src="js/main.js"></script>
    
  </body>
</html>